{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "from pyspark.sql.types import StringType\nimport pyspark.sql.functions as f\nimport pandas as pd\nfrom collections import Counter", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Starting Spark application\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1551986734745_0003</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-53-199.ec2.internal:20888/proxy/application_1551986734745_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-50-222.ec2.internal:8042/node/containerlogs/container_1551986734745_0003_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "text": "SparkSession available as 'spark'.\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "a = list('A') * 5\nb = list('B') * 2\nc = list('C') * 3\nd = list('D') * 2\nl = a + b + c + d\nl", "execution_count": 24, "outputs": [{"output_type": "stream", "text": "['A', 'A', 'A', 'A', 'A', 'B', 'B', 'C', 'C', 'C', 'D', 'D']", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "races = ['w', 'w', 'b', 'u', 'w', 'b', 'u', 'u', 'u', 'w', 'u', 'u']", "execution_count": 31, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "data = {\"guid\": l, \"race\": races}", "execution_count": 32, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df = pd.DataFrame(data)", "execution_count": 33, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df = spark.createDataFrame(df)", "execution_count": 34, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.show()", "execution_count": 35, "outputs": [{"output_type": "stream", "text": "+----+----+\n|guid|race|\n+----+----+\n|   A|   w|\n|   A|   w|\n|   A|   b|\n|   A|   u|\n|   A|   w|\n|   B|   b|\n|   B|   u|\n|   C|   u|\n|   C|   u|\n|   C|   w|\n|   D|   u|\n|   D|   u|\n+----+----+", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.groupBy(\"guid\").agg(f.collect_list(f.col(\"race\")).alias('race_list')).show()", "execution_count": 36, "outputs": [{"output_type": "stream", "text": "+----+---------------+\n|guid|      race_list|\n+----+---------------+\n|   B|         [b, u]|\n|   D|         [u, u]|\n|   C|      [u, u, w]|\n|   A|[w, w, b, u, w]|\n+----+---------------+", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "@f.udf(StringType())\ndef get_race(col):\n    # change function to address ties and unknown as top most common\n    race = Counter(col).most_common(7)[0][0] # 7 race categories\n    return race", "execution_count": 48, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# test function\nprint(races)\nd = Counter(races)\nprint(d)\nd.most_common(7)[0][0] # get top most common -- still need to address ties and unknown as top most common", "execution_count": 45, "outputs": [{"output_type": "stream", "text": "['w', 'w', 'b', 'u', 'w', 'b', 'u', 'u', 'u', 'w', 'u', 'u']\nCounter({'u': 6, 'w': 4, 'b': 2})\n'u'", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df\\\n    .groupBy(\"guid\").agg(f.collect_list(f.col(\"race\")).alias('race_list'))\\\n    .withColumn(\"race_census\", get_race(f.col('race_list')))\\\n    .show()", "execution_count": 46, "outputs": [{"output_type": "stream", "text": "+----+---------------+-----------+\n|guid|      race_list|race_census|\n+----+---------------+-----------+\n|   B|         [b, u]|          b|\n|   D|         [u, u]|          u|\n|   C|      [u, u, w]|          u|\n|   A|[w, w, b, u, w]|          w|\n+----+---------------+-----------+", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Note there is only one data shuffle needed (group by Exchange) and the data never gets bigger\ndf\\\n    .groupBy(\"guid\").agg(f.collect_list(f.col(\"race\")).alias('race_list'))\\\n    .withColumn(\"race_census\", get_race(f.col('race_list')))\\\n    .explain()", "execution_count": 47, "outputs": [{"output_type": "stream", "text": "== Physical Plan ==\n*(1) Project [guid#76, race_list#129, pythonUDF0#137 AS race_census#133]\n+- BatchEvalPython [get_race(race_list#129)], [guid#76, race_list#129, pythonUDF0#137]\n   +- ObjectHashAggregate(keys=[guid#76], functions=[collect_list(race#77, 0, 0)])\n      +- Exchange hashpartitioning(guid#76, 200)\n         +- ObjectHashAggregate(keys=[guid#76], functions=[partial_collect_list(race#77, 0, 0)])\n            +- Scan ExistingRDD[guid#76,race#77]", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pyspark3kernel", "display_name": "PySpark3", "language": ""}, "language_info": {"name": "pyspark3", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 3}, "pygments_lexer": "python3"}}, "nbformat": 4, "nbformat_minor": 2}